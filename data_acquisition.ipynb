{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Article Data\n",
    "\n",
    "## Getting the Article, Population and Region Data\n",
    "\n",
    "The first step is getting the data, which lives in several different places. We collect the data that lists the Wikipedia articles about US Cities and data for US state populations.\n",
    "\n",
    "[The Wikipedia Category: Lists of cities in the United States by State](https://en.m.wikipedia.org/wiki/Category:Lists_of_cities_in_the_United_States_by_state) was crawled to generate a list of wikipedia article pages about US cities from each state. Location of the data in the homework folder: `input-data/us_cities_by_state_SEPT.2023.csv`\n",
    "\n",
    "The US Census Bureau provides updated population estimates for every US state. The data for the population is taken from the State's [website](https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html) and stored in the homework folder in `input-data/NST-EST2022-POP.xlsx`.\n",
    "\n",
    "The region demarcation within the US is not one standardized and fixed thing. For this homework, we are using the regional and divisional agglomerations as defined by the US Census Bureau. The spreadsheet listing the states in each regional division is in the homework folder in `input-data/US States by Region - US Census Bureau.xlsx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- importing libraries --------------------------- #\n",
    "import json, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# to get a token for api\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- importing the input datasets ----------------------- #\n",
    "\n",
    "df_cities_state = pd.read_csv(\"input-data/us_cities_by_state_SEPT.2023.csv\")\n",
    "df_population = pd.read_excel(\"input-data/NST-EST2022-POP.xlsx\",\n",
    "                              header=None,\n",
    "                              skiprows=9,\n",
    "                              names=['State','Base 2020', '2020 Estimate', '2021 Estimate', '2022 Estimate'],\n",
    "                              skipfooter=5)\n",
    "df_regions = pd.read_excel(\"input-data/US States by Region - US Census Bureau.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the input data for any inconsistencies\n",
    "\n",
    "Crawling Wikipedia categories to identify relevant page subsets can result in misleading and/or duplicate category labels. Naturally, the data crawl attempted to resolve these, but not all may have been caught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistencies in wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state           page_title  \\\n",
       "0  Alabama   Abbeville, Alabama   \n",
       "1  Alabama  Adamsville, Alabama   \n",
       "2  Alabama     Addison, Alabama   \n",
       "3  Alabama       Akron, Alabama   \n",
       "4  Alabama   Alabaster, Alabama   \n",
       "\n",
       "                                                 url  \n",
       "0   https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1  https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2     https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3       https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4   https://en.wikipedia.org/wiki/Alabaster,_Alabama  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1260 duplicate records.\n",
      "Shape of the data: (22157, 3)\n",
      "Missing values: state         0\n",
      "page_title    0\n",
      "url           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- checking for duplicates ------------------------- #\n",
    "\n",
    "duplicate_records = df_cities_state[df_cities_state.duplicated(subset=['state', 'page_title', 'url'], keep = False)]\n",
    "\n",
    "print(f'''There are a total of {duplicate_records.shape[0]} duplicate records.''')\n",
    "print(f'Shape of the data: {df_cities_state.shape}')\n",
    "print(f'Missing values: {df_cities_state.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1260 records that have the same state, page_title and url values. As they are exactly the same, we drop the duplicate records and keep only the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping duplicates: (21525, 3)\n"
     ]
    }
   ],
   "source": [
    "# dropping the duplicate records\n",
    "df_cities_state = df_cities_state[~df_cities_state.duplicated(subset = ['state', 'page_title', 'url'], keep = 'last')]\n",
    "print(f'Shape after dropping duplicates: {df_cities_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistencies in population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Base 2020</th>\n",
       "      <th>2020 Estimate</th>\n",
       "      <th>2021 Estimate</th>\n",
       "      <th>2022 Estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.Alabama</td>\n",
       "      <td>5024356.0</td>\n",
       "      <td>5031362.0</td>\n",
       "      <td>5049846.0</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.Alaska</td>\n",
       "      <td>733378.0</td>\n",
       "      <td>732923.0</td>\n",
       "      <td>734182.0</td>\n",
       "      <td>733583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.Arizona</td>\n",
       "      <td>7151507.0</td>\n",
       "      <td>7179943.0</td>\n",
       "      <td>7264877.0</td>\n",
       "      <td>7359197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.Arkansas</td>\n",
       "      <td>3011555.0</td>\n",
       "      <td>3014195.0</td>\n",
       "      <td>3028122.0</td>\n",
       "      <td>3045637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.California</td>\n",
       "      <td>39538245.0</td>\n",
       "      <td>39501653.0</td>\n",
       "      <td>39142991.0</td>\n",
       "      <td>39029342.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         State   Base 2020  2020 Estimate  2021 Estimate  2022 Estimate\n",
       "0     .Alabama   5024356.0      5031362.0      5049846.0      5074296.0\n",
       "1      .Alaska    733378.0       732923.0       734182.0       733583.0\n",
       "2     .Arizona   7151507.0      7179943.0      7264877.0      7359197.0\n",
       "3    .Arkansas   3011555.0      3014195.0      3028122.0      3045637.0\n",
       "4  .California  39538245.0     39501653.0     39142991.0     39029342.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9r/7q8l4tjd43z8stsvm3s_5g400000gn/T/ipykernel_53271/2148530196.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_population['State'] = df_population['State'].str.replace(\".\", \"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State            1\n",
       "Base 2020        1\n",
       "2020 Estimate    1\n",
       "2021 Estimate    1\n",
       "2022 Estimate    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the '.' that appears before each state name\n",
    "df_population['State'] = df_population['State'].str.replace(\".\", \"\")\n",
    "df_population.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that there is one null value in each column. This is because when you eyeball the dataset there is a blank row. As there is no data in any column, we drop the entire row from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape of data: (53, 5)\n",
      "Shape after dropping null values: (52, 5)\n",
      "Duplicate Values: 0\n"
     ]
    }
   ],
   "source": [
    "# ----------------- dropping the null values from the dataset ---------------- #\n",
    "\n",
    "print(f'Original Shape of data: {df_population.shape}')\n",
    "df_population.dropna(inplace = True)\n",
    "print(f'Shape after dropping null values: {df_population.shape}')\n",
    "print(f'Duplicate Values: {df_population.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inconsistencies in regions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>New England</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Connecticut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      REGION     DIVISION          STATE\n",
       "0  Northeast          NaN            NaN\n",
       "1        NaN  New England            NaN\n",
       "2        NaN          NaN    Connecticut\n",
       "3        NaN          NaN          Maine\n",
       "4        NaN          NaN  Massachusetts"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_regions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fix the structure of the df_regions we use the forward fill method to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Connecticut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Rhode Island</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      REGION     DIVISION          STATE\n",
       "0  Northeast  New England    Connecticut\n",
       "1  Northeast  New England          Maine\n",
       "2  Northeast  New England  Massachusetts\n",
       "3  Northeast  New England  New Hampshire\n",
       "4  Northeast  New England   Rhode Island"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------ restructing the data using forward fill ----------------- #\n",
    "\n",
    "df_regions['REGION'].fillna(method = 'ffill', inplace = True)\n",
    "df_regions['DIVISION'].fillna(method = 'ffill', inplace = True)\n",
    "df_regions['STATE'].fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "# only keeping rows where state is not null\n",
    "df_regions = df_regions[df_regions['STATE'].notnull()]\n",
    "df_regions.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df_regions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checked for consistences in all the three input data sets and handled them by dropping the duplicates and null values. We also fixed the structure of the regions data to not have any NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Article Quality Predictions\n",
    "\n",
    "Now we need to get the predicted quality scores for each article in the Wikipedia dataset. We're using a machine learning system called ORES. This was originally an acronym for \"Objective Revision Evaluation Service\" but was simply renamed “ORES”. ORES is a machine learning tool that can provide estimates of Wikipedia article quality. The article quality estimates are, from best to worst:\n",
    "\n",
    "- FA: Featured article\n",
    "- GA: Good article\n",
    "- B: B-class article\n",
    "- C: C-class article\n",
    "- Start: Start-class article\n",
    "- Stub: Stub-class article\n",
    "\n",
    "These were learned based on articles in Wikipedia that were peer-reviewed using the Wikipedia content assessment procedures.These quality classes are a sub-set of quality assessment categories developed by Wikipedia editors.\n",
    "\n",
    "ORES requires a specific revision ID of a specific article to be able to make a label prediction. We are going to use the API: Info request to get a range of metadata on an article, including the most current revision ID of the article page. \n",
    "\n",
    "Putting this together, to get a Wikipedia page quality prediction from ORES for each politician’s article page you will need to: \n",
    "\n",
    "1. read each line of us_cities_by_state_SEPT.2023.csv\n",
    "2. make a page info request to get the current page revision\n",
    "3. make an ORES request using the page title and current revision id.\n",
    "\n",
    "The homework folder contains example code in notebooks to illustrate making a page info request and making an ORES request. This sample code is licensed CC0 so feel free to reuse any of the code in either notebook without attribution. \n",
    "\n",
    "Note: It is possible that we will be unable to get a score for a particular article. If that happens, we are going to maintain a log of articles for which we were not able to retrieve an ORES score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Info API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- code snippet sourced from Dr. David W. McDonald's notebook -------- #\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<nsaumya@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": None,\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- code snippet sourced from Dr. David W. McDonald's notebook -------- #\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None,\n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT,\n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "\n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- defining the article titles for the function --------------- #\n",
    "\n",
    "ARTICLE_TITLES = df_cities_state['page_title'].replace(\" \", \"_\")\n",
    "cities_info_filename = \"intermediate-data/cities-info.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- running the api function for each article title ------------- #\n",
    "\n",
    "cities_info = {}\n",
    "for title in ARTICLE_TITLES[0:]:\n",
    "  print(f\"Getting page info data for: {title}\")\n",
    "  request_info = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "  request_info['titles'] = title\n",
    "  cities_info[title] = request_pageinfo_per_article(request_template = request_info)\n",
    "\n",
    "with open(cities_info_filename, \"w\") as cities_data:\n",
    "  cities_data.write(json.dumps(cities_info, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for the above cell is cleared as the output had a lot of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Quality Prediction API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the JSON file saved from the previous step\n",
    "with open(\"intermediate-data/cities-info.json\", 'r') as json_file:\n",
    "    cities_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- converting to a dataframe ------------------------ #\n",
    "\n",
    "city_data_list = []\n",
    "for city, city_info in cities_data.items():\n",
    "    page_data = city_info[\"query\"][\"pages\"]\n",
    "    for page_id, page_info in page_data.items():\n",
    "        city_data = {\n",
    "            \"pageid\": page_info[\"pageid\"],\n",
    "            \"ns\": page_info[\"ns\"],\n",
    "            \"title\": page_info[\"title\"],\n",
    "            \"contentmodel\": page_info[\"contentmodel\"],\n",
    "            \"pagelanguage\": page_info[\"pagelanguage\"],\n",
    "            \"pagelanguagehtmlcode\": page_info[\"pagelanguagehtmlcode\"],\n",
    "            \"pagelanguagedir\": page_info[\"pagelanguagedir\"],\n",
    "            \"touched\": page_info[\"touched\"],\n",
    "            \"lastrevid\": page_info[\"lastrevid\"],\n",
    "            \"length\": page_info[\"length\"],\n",
    "            \"fullurl\": page_info[\"fullurl\"],\n",
    "            \"editurl\": page_info[\"editurl\"],\n",
    "            \"canonicalurl\": page_info[\"canonicalurl\"]\n",
    "        }\n",
    "        city_data_list.append(city_data)\n",
    "\n",
    "# creating a DataFrame\n",
    "df = pd.DataFrame(city_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>ns</th>\n",
       "      <th>title</th>\n",
       "      <th>contentmodel</th>\n",
       "      <th>pagelanguage</th>\n",
       "      <th>pagelanguagehtmlcode</th>\n",
       "      <th>pagelanguagedir</th>\n",
       "      <th>touched</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>length</th>\n",
       "      <th>fullurl</th>\n",
       "      <th>editurl</th>\n",
       "      <th>canonicalurl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104730</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>24706</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Abb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104761</td>\n",
       "      <td>0</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>18040</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Ada...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105188</td>\n",
       "      <td>0</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>13309</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Add...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104726</td>\n",
       "      <td>0</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>11710</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Akr...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105109</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>20343</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Ala...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid  ns                title contentmodel pagelanguage  \\\n",
       "0  104730   0   Abbeville, Alabama     wikitext           en   \n",
       "1  104761   0  Adamsville, Alabama     wikitext           en   \n",
       "2  105188   0     Addison, Alabama     wikitext           en   \n",
       "3  104726   0       Akron, Alabama     wikitext           en   \n",
       "4  105109   0   Alabaster, Alabama     wikitext           en   \n",
       "\n",
       "  pagelanguagehtmlcode pagelanguagedir               touched   lastrevid  \\\n",
       "0                   en             ltr  2023-10-10T22:35:37Z  1171163550   \n",
       "1                   en             ltr  2023-10-10T22:35:37Z  1177621427   \n",
       "2                   en             ltr  2023-10-10T22:35:37Z  1168359898   \n",
       "3                   en             ltr  2023-10-10T22:35:37Z  1165909508   \n",
       "4                   en             ltr  2023-10-10T22:35:37Z  1179139816   \n",
       "\n",
       "   length                                            fullurl  \\\n",
       "0   24706   https://en.wikipedia.org/wiki/Abbeville,_Alabama   \n",
       "1   18040  https://en.wikipedia.org/wiki/Adamsville,_Alabama   \n",
       "2   13309     https://en.wikipedia.org/wiki/Addison,_Alabama   \n",
       "3   11710       https://en.wikipedia.org/wiki/Akron,_Alabama   \n",
       "4   20343   https://en.wikipedia.org/wiki/Alabaster,_Alabama   \n",
       "\n",
       "                                             editurl  \\\n",
       "0  https://en.wikipedia.org/w/index.php?title=Abb...   \n",
       "1  https://en.wikipedia.org/w/index.php?title=Ada...   \n",
       "2  https://en.wikipedia.org/w/index.php?title=Add...   \n",
       "3  https://en.wikipedia.org/w/index.php?title=Akr...   \n",
       "4  https://en.wikipedia.org/w/index.php?title=Ala...   \n",
       "\n",
       "                                        canonicalurl  \n",
       "0   https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1  https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2     https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3       https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4   https://en.wikipedia.org/wiki/Alabaster,_Alabama  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- .env for storing sensitive api token information ------------- #\n",
    "\n",
    "wiki_tokens = dotenv_values(\".env\")\n",
    "USERNAME = wiki_tokens['USERNAME']\n",
    "ACCESS_TOKEN = wiki_tokens['ACCESS_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- code snippet sourced from Dr. David W. McDonald's notebook -------- #\n",
    "\n",
    "# The current LiftWing ORES API endpoint and prediction model\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "# The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "# come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "#\n",
    "# Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "# as part of the header too\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"nsaumya@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "# This is a template for the parameters that we need to supply in the headers of an API request\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"saumya.nauni@gmail.com\",\n",
    "    'access_token'  : ACCESS_TOKEN\n",
    "}\n",
    "\n",
    "# This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- code snippet sourced from Dr. David W. McDonald's notebook -------- #\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT,\n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL,\n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE,\n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE,\n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "\n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "\n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "\n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "\n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "        prediction = json_response['enwiki']['scores'][str(article_revid)]['articlequality']['score']['prediction']\n",
    "        prediction_score = json_response['enwiki']['scores'][str(article_revid)]['articlequality']['score']['probability'][prediction]\n",
    "        if type(prediction_score) is not float:\n",
    "            print(\"Prediction score is not a float\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        prediction, prediction_score = (None, None)\n",
    "    return (prediction, prediction_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- iterating through the dataframe to get the quality and article score --- #\n",
    "\n",
    "# defining two column series\n",
    "predictions = pd.Series(dtype='object')\n",
    "prediction_scores = pd.Series(dtype='float64')\n",
    "\n",
    "# iterating through the DataFrame and using the request ores function to get the quality score and article quality\n",
    "for index, row in df.iterrows():\n",
    "    print(row['title'])\n",
    "    article_revid = row['lastrevid']\n",
    "    prediction, prediction_score = request_ores_score_per_article(article_revid=article_revid)\n",
    "\n",
    "    # appending the results to the Series\n",
    "    predictions.at[index] = prediction\n",
    "    prediction_scores.at[index] = prediction_score\n",
    "\n",
    "# adding the series to the DataFrame\n",
    "df['article_quality'] = predictions\n",
    "df['quality_score'] = prediction_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for the above cell is cleared as the output had a lot of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>ns</th>\n",
       "      <th>title</th>\n",
       "      <th>contentmodel</th>\n",
       "      <th>pagelanguage</th>\n",
       "      <th>pagelanguagehtmlcode</th>\n",
       "      <th>pagelanguagedir</th>\n",
       "      <th>touched</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>length</th>\n",
       "      <th>fullurl</th>\n",
       "      <th>editurl</th>\n",
       "      <th>canonicalurl</th>\n",
       "      <th>article_quality</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104730</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>24706</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Abb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>0.59792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104761</td>\n",
       "      <td>0</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>18040</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Ada...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>0.37707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105188</td>\n",
       "      <td>0</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>13309</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Add...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>0.32446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104726</td>\n",
       "      <td>0</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>11710</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Akr...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.448584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105109</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>wikitext</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ltr</td>\n",
       "      <td>2023-10-10T22:35:37Z</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>20343</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Ala...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid  ns                title contentmodel pagelanguage  \\\n",
       "0  104730   0   Abbeville, Alabama     wikitext           en   \n",
       "1  104761   0  Adamsville, Alabama     wikitext           en   \n",
       "2  105188   0     Addison, Alabama     wikitext           en   \n",
       "3  104726   0       Akron, Alabama     wikitext           en   \n",
       "4  105109   0   Alabaster, Alabama     wikitext           en   \n",
       "\n",
       "  pagelanguagehtmlcode pagelanguagedir               touched   lastrevid  \\\n",
       "0                   en             ltr  2023-10-10T22:35:37Z  1171163550   \n",
       "1                   en             ltr  2023-10-10T22:35:37Z  1177621427   \n",
       "2                   en             ltr  2023-10-10T22:35:37Z  1168359898   \n",
       "3                   en             ltr  2023-10-10T22:35:37Z  1165909508   \n",
       "4                   en             ltr  2023-10-10T22:35:37Z  1179139816   \n",
       "\n",
       "   length                                            fullurl  \\\n",
       "0   24706   https://en.wikipedia.org/wiki/Abbeville,_Alabama   \n",
       "1   18040  https://en.wikipedia.org/wiki/Adamsville,_Alabama   \n",
       "2   13309     https://en.wikipedia.org/wiki/Addison,_Alabama   \n",
       "3   11710       https://en.wikipedia.org/wiki/Akron,_Alabama   \n",
       "4   20343   https://en.wikipedia.org/wiki/Alabaster,_Alabama   \n",
       "\n",
       "                                             editurl  \\\n",
       "0  https://en.wikipedia.org/w/index.php?title=Abb...   \n",
       "1  https://en.wikipedia.org/w/index.php?title=Ada...   \n",
       "2  https://en.wikipedia.org/w/index.php?title=Add...   \n",
       "3  https://en.wikipedia.org/w/index.php?title=Akr...   \n",
       "4  https://en.wikipedia.org/w/index.php?title=Ala...   \n",
       "\n",
       "                                        canonicalurl article_quality  \\\n",
       "0   https://en.wikipedia.org/wiki/Abbeville,_Alabama               C   \n",
       "1  https://en.wikipedia.org/wiki/Adamsville,_Alabama               C   \n",
       "2     https://en.wikipedia.org/wiki/Addison,_Alabama               C   \n",
       "3       https://en.wikipedia.org/wiki/Akron,_Alabama              GA   \n",
       "4   https://en.wikipedia.org/wiki/Alabaster,_Alabama               C   \n",
       "\n",
       "  quality_score  \n",
       "0       0.59792  \n",
       "1       0.37707  \n",
       "2       0.32446  \n",
       "3      0.448584  \n",
       "4      0.646384  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed requests: 3817\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- count of NA ------------------------------- #\n",
    "\n",
    "df_failed = df[df['quality_score'].isna()]\n",
    "print(f\"Failed requests: {len(df_failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the number of failed requests is very high we run the ORES API call again only on the failed data rows to update them. The goal of this step is to retry the failed requests and check if they get the article and quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- iterating on failed requests dataframe to get the quality and article score --- #\n",
    "\n",
    "# defining a function that reruns the ORES API on the failed dataframe and updates the original one\n",
    "def update_row(row):\n",
    "    print(row['title'])\n",
    "    article_revid = row['lastrevid']\n",
    "    prediction, prediction_score = request_ores_score_per_article(article_revid = article_revid)\n",
    "    row['article_quality'] = prediction\n",
    "    row['quality_score'] = prediction_score\n",
    "    return row\n",
    "\n",
    "# using the apply method to update each row in df_failed and assign it back to the DataFrame\n",
    "df_failed = df_failed.apply(update_row, axis=1)\n",
    "\n",
    "# updating the original dataframe\n",
    "for index, row in df_failed.iterrows():\n",
    "    df.at[index, 'article_quality'] = row['article_quality']\n",
    "    df.at[index, 'quality_score'] = row['quality_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for the above cell is cleared as the output had a lot of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed requests: 0\n"
     ]
    }
   ],
   "source": [
    "# reconfirming the number of failed requests (the numbers should be 0)\n",
    "df_failed = df[df['quality_score'].isna()]\n",
    "print(f\"Failed requests: {len(df_failed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pageid                  0.0\n",
       "ns                      0.0\n",
       "title                   0.0\n",
       "contentmodel            0.0\n",
       "pagelanguage            0.0\n",
       "pagelanguagehtmlcode    0.0\n",
       "pagelanguagedir         0.0\n",
       "touched                 0.0\n",
       "lastrevid               0.0\n",
       "length                  0.0\n",
       "fullurl                 0.0\n",
       "editurl                 0.0\n",
       "canonicalurl            0.0\n",
       "article_quality         0.0\n",
       "quality_score           0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_failed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no failed requests we proceed with saving the dataframe and performing the combining steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe\n",
    "df.to_csv(\"intermediate-data/cities_article_quality.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Datasets\n",
    "\n",
    "Some processing of the data will be necessary. In particular, merging the wikipedia data and population data together on state names. The combined dataset also requires labeling each state with it's US Census regional-division. The dataset `df_regions` represents regions, division and states hierarchically. We merge this dataset with the combined dataset. When merging the data there will be some non-states (Washington, DC or Puerto Rico), we ignore these non-states. We identify all areas for which there are no matches and output a list naming those areas, with each area on a separate line.\n",
    "\n",
    "Lastly we consolidate the merged data into a single csv file: `data/wp_scored_city_articles_by_state.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"intermediate-data/cities_article_quality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C        12919\n",
       "GA        4731\n",
       "Start     2101\n",
       "B          880\n",
       "Stub       678\n",
       "FA         210\n",
       "Name: article_quality, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- counting the count of classes in the dataframe -------------- #\n",
    "\n",
    "df['article_quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- modifying the dataframe ------------------------- #\n",
    "\n",
    "# selecting only the columns that are required for the analysis\n",
    "df = df[['title', 'lastrevid', 'article_quality']]\n",
    "# renaming the columns for a more descriptive header\n",
    "df.columns = ['article_title', 'revision_id', 'article_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         article_title  revision_id article_quality    state\n",
       "0   Abbeville, Alabama   1171163550               C  Alabama\n",
       "1  Adamsville, Alabama   1177621427               C  Alabama\n",
       "2     Addison, Alabama   1168359898               C  Alabama\n",
       "3       Akron, Alabama   1165909508              GA  Alabama\n",
       "4   Alabaster, Alabama   1179139816               C  Alabama"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------- getting state names in the dataframe ------------------- #\n",
    "\n",
    "# in order to merge the datasets we derive state name by joining on the page title found in the original dataset\n",
    "df = df.merge(df_cities_state[['page_title', 'state']], left_on = 'article_title', right_on = 'page_title', how = 'left')\n",
    "# dropping the page title column\n",
    "df = df.drop(columns=['page_title'])\n",
    "\n",
    "# -------------------- modifying the values in the columns ------------------- #\n",
    "\n",
    "# replacing '_' with space in the state column (original form)\n",
    "df['state'] = df['state'].str.replace(\"_\", \" \")\n",
    "\n",
    "# replacing 'Georgia (U.S. state)' with 'Georgia' to match the state name in df_regions\n",
    "df.loc[df['state'] == 'Georgia (U.S. state)', 'state'] = 'Georgia'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "      <th>state</th>\n",
       "      <th>2022 Estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         article_title  revision_id article_quality    state  2022 Estimate\n",
       "0   Abbeville, Alabama   1171163550               C  Alabama      5074296.0\n",
       "1  Adamsville, Alabama   1177621427               C  Alabama      5074296.0\n",
       "2     Addison, Alabama   1168359898               C  Alabama      5074296.0\n",
       "3       Akron, Alabama   1165909508              GA  Alabama      5074296.0\n",
       "4   Alabaster, Alabama   1179139816               C  Alabama      5074296.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- merging wikipedia data with the population data ------------- #\n",
    "\n",
    "# keeping only state name and 2022 estimate columns required for the analysis\n",
    "df_population = df_population[['State', '2022 Estimate']]\n",
    "\n",
    "# merging df with df_population on the state name\n",
    "df = df.merge(df_population, left_on = 'state', right_on = 'State', how = 'left')\n",
    "\n",
    "df = df.drop(columns = ['State'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- merging the regions datafrane with the combined dataframe ------------ #\n",
    "\n",
    "# mergeing the combined dataframes on the regions dataframe to get the region and division\n",
    "df = df.merge(df_regions[['DIVISION','STATE']], left_on = 'state', right_on = 'STATE', how = 'left')\n",
    "\n",
    "# droping the STATE column to avoid duplication\n",
    "df = df.drop(columns=['STATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "      <th>state</th>\n",
       "      <th>2022 Estimate</th>\n",
       "      <th>DIVISION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>East South Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         article_title  revision_id article_quality    state  2022 Estimate  \\\n",
       "0   Abbeville, Alabama   1171163550               C  Alabama      5074296.0   \n",
       "1  Adamsville, Alabama   1177621427               C  Alabama      5074296.0   \n",
       "2     Addison, Alabama   1168359898               C  Alabama      5074296.0   \n",
       "3       Akron, Alabama   1165909508              GA  Alabama      5074296.0   \n",
       "4   Alabaster, Alabama   1179139816               C  Alabama      5074296.0   \n",
       "\n",
       "             DIVISION  \n",
       "0  East South Central  \n",
       "1  East South Central  \n",
       "2  East South Central  \n",
       "3  East South Central  \n",
       "4  East South Central  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state   regional_division  population        article_title  revision_id  \\\n",
       "0  Alabama  East South Central   5074296.0   Abbeville, Alabama   1171163550   \n",
       "1  Alabama  East South Central   5074296.0  Adamsville, Alabama   1177621427   \n",
       "2  Alabama  East South Central   5074296.0     Addison, Alabama   1168359898   \n",
       "3  Alabama  East South Central   5074296.0       Akron, Alabama   1165909508   \n",
       "4  Alabama  East South Central   5074296.0   Alabaster, Alabama   1179139816   \n",
       "\n",
       "  article_quality  \n",
       "0               C  \n",
       "1               C  \n",
       "2               C  \n",
       "3              GA  \n",
       "4               C  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- renaming and reordering the columns ------------------- #\n",
    "\n",
    "df = df.rename(columns={'DIVISION': 'regional_division', '2022 Estimate': 'population'})\n",
    "df = df[['state', 'regional_division', 'population', 'article_title', 'revision_id', 'article_quality']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2010 United States census\n",
      "Dropped 2010 United States census\n",
      "Dropped 2010 United States census\n",
      "Dropped 2010 United States census\n",
      "Dropped 2020 United States census\n",
      "Dropped 2020 United States census\n",
      "Dropped 2020 United States census\n",
      "Dropped 2020 United States census\n",
      "Dropped County (United States)\n",
      "Dropped County (United States)\n",
      "Dropped County (United States)\n",
      "Dropped County (United States)\n",
      "Dropped County (United States)\n",
      "Dropped County (United States)\n",
      "Dropped Population\n",
      "Dropped Population\n",
      "Dropped Population\n",
      "Dropped Population\n",
      "Dropped Square mile\n",
      "Dropped Federal Information Processing Standards\n",
      "Dropped American National Standards Institute\n",
      "Dropped Geographic Names Information System\n",
      "Dropped Wikipedia:Citation needed\n",
      "Dropped Wikipedia:Citation needed\n"
     ]
    }
   ],
   "source": [
    "# ------------------ removing non-states from article_title ------------------ #\n",
    "\n",
    "articles_to_remove = ['Federal Information Processing Standards',\n",
    " 'American National Standards Institute',\n",
    " 'Geographic Names Information System',\n",
    " 'Wikipedia:Citation needed',\n",
    " '2020 United States census',\n",
    " '2010 United States census',\n",
    " 'County (United States)',\n",
    " 'Population',\n",
    " 'Square mile']\n",
    "for index, row in df.iterrows():\n",
    "    if row['article_title'] in articles_to_remove:\n",
    "        df.drop(index, inplace=True)\n",
    "        print(f\"Dropped {row['article_title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4088"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------- checking for duplicate values ---------------------- #\n",
    "\n",
    "duplicates = df[df.duplicated()]\n",
    "sum_of_duplicates = duplicates.shape[0]\n",
    "sum_of_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are a large number of duplicates, we drop the duplicate values that have the same values across all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------- dropping duplicate values ------------------------ #\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#rechecking duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "duplicates.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state   regional_division  population        article_title  revision_id  \\\n",
       "0  Alabama  East South Central   5074296.0   Abbeville, Alabama   1171163550   \n",
       "1  Alabama  East South Central   5074296.0  Adamsville, Alabama   1177621427   \n",
       "2  Alabama  East South Central   5074296.0     Addison, Alabama   1168359898   \n",
       "3  Alabama  East South Central   5074296.0       Akron, Alabama   1165909508   \n",
       "4  Alabama  East South Central   5074296.0   Alabaster, Alabama   1179139816   \n",
       "\n",
       "  article_quality  \n",
       "0               C  \n",
       "1               C  \n",
       "2               C  \n",
       "3              GA  \n",
       "4               C  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/wp_scored_city_articles_by_state.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
